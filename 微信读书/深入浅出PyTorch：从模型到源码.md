---
doc_type: weread-highlights-reviews
bookId: "30638126"
author: 张校捷
cover: https://cdn.weread.qq.com/weread/cover/47/YueWen_30638126/t7_YueWen_30638126.jpg
reviewCount: 0
noteCount: 12
isbn: 9787121386411
category: 计算机-编程设计
lastReadDate: 2021-02-06
---
# 元数据
> [!abstract] 深入浅出PyTorch：从模型到源码
> - ![ 深入浅出PyTorch：从模型到源码|200](https://cdn.weread.qq.com/weread/cover/47/YueWen_30638126/t7_YueWen_30638126.jpg)
> - 书名： 深入浅出PyTorch：从模型到源码
> - 作者： 张校捷
> - 简介： 本书从机器学习和深度学习的基础概念入手，由浅到深地详细介绍了PyTorch深度学习框架的知识，主要包含深度学习的基础知识，如神经网络的优化算法、神经网络的模块等；同时也包含了深度学习的进阶知识，如使用PyTorch构建复杂的深度学习模型，以及前沿的深度学习模型的介绍等。另外，为了加深读者对PyTorch深度学习框架的理解和掌握，本书还介绍了PyTorch的源代码结构，包括该框架的Python语言前端和C++语言后端的源代码结构。
> - 出版时间 2020-03-01 00:00:00
> - ISBN： 9787121386411
> - 分类： 计算机-编程设计
> - 出版社： 电子工业出版社

# 高亮划线

## 第5章 PyTorch自然语言处理模块


- 📌 5.1 自然语言处理基本概念自然语言处理（Natural Language Processing，NLP）研究的主要是如何通过计算机算法来理解自然语言。相比于计算机视觉的问题，自然语言处理的问题具有自身的一些特点。机器视觉问题的输入主要是各种图像，这些图像可以视作真实世界物理规则的一种映射，比如图像可以看作是由像素构成的，像素可以看作由三个原色叠加而成，每个原色具有自身的对比度（也就是像素值），这一点就构成了计算机视觉的基础，也构成了把图像数字化和后续的通过深度学习模型来处理图像的基础。 ^30638126-49-443
    - ⏱ 2020-08-23 00:58:22 

- 📌 神经网络机器翻译模型相对于统计机器学习模型来说，增加了模型的容量，而且更多地考虑了上下文和语义的结构，翻译的结果更准确，模型的训练也更简单。 ^30638126-49-3332-3402
    - ⏱ 2020-08-23 11:24:45 

- 📌 时间和地点的信息（命名实体识别，Named EntityRecognition，NER）。 ^30638126-49-3589-3635
    - ⏱ 2020-08-23 11:25:10 

- 📌 对于英文，可以把句子按照单词进行划分，把文本分割成单词的列表，这部分称为分词（Tokenization） ^30638126-49-4248-4300
    - ⏱ 2020-08-23 11:27:52 

- 📌 下一步需要做的是去掉停用词（Stopwords），在自然语言处理的任务中，很多单词对于语义没有贡献，在某些任务中就可以忽略掉这些单词，这些单词被称为停用词，去掉这些停用词能够消除文本的冗余，提高模型的准确率。常见的停用词在英语中有“the”“of”等 ^30638126-49-4398-4523
    - ⏱ 2020-08-23 11:28:09 

- 📌 即单词频率-逆文档频率特征。这个特征提取方法的基本思想是，单词的出现频率有时候并不是一个很好的代表文档特征的值。 ^30638126-49-8760-8816
    - ⏱ 2020-08-23 11:34:41 
## 5.2 词嵌入层


- 📌 （可以通过使用稀疏矩阵编码来解决）。 ^30638126-50-1567-1585
    - ⏱ 2020-08-23 00:50:51 

- 📌 第二个缺点是在独热编码的条件下，两个单词之间的关系是正交的，也就是任意两个不同的单词，其内积为0，这一点不利于寻找单词之间的相似性。 ^30638126-50-1614-1680
    - ⏱ 2020-08-23 12:41:19 

- 📌 不会做矩阵乘法，而是直接使用N×M的矩阵，称为词嵌入矩阵（Embedding Matrix），通过取出对应元素索引序号的行，来获取某个元素对应的词向量。 ^30638126-50-2168-2300
    - ⏱ 2020-08-25 21:08:29 

- 📌 num_embeddings参数代表单词表的单词数目。embedding_dim参数代表词嵌入层输出词向量的维度大小。 ^30638126-50-3098-3186
    - ⏱ 2020-08-25 21:10:35 
## 5.4 注意力机制


- 📌 ，RNN（包括LSTM和GRU，它们只是在设计上尽量减少遗忘的发生）都不可避免地出现遗忘的状况 ^30638126-52-1436-1483
    - ⏱ 2021-02-06 01:35:54 

- 📌 所谓注意力机制，就是通过引入一个神经网络，计算编码器的输出对解码器贡献的权重，最后计算加权平均后编码器的输出，即上下文（Context），通过在编码器的输出和下一步的输入中引入上下文的信息，最后达到让解码器的某一个特定的解码和编码器的一些输出关联起来，即对齐（Alignment）的效果 ^30638126-52-1553-1696
    - ⏱ 2021-02-06 01:36:14 

- 📌 在计算权重的时候有个技巧，就是对填充单词＜PAD＞对应的权重，需要将对应的分数设为负无穷大-inf，这样对应的Softmax的权重为0。因此，就达到了不考虑填充单词的目的。 ^30638126-52-3256-3342
    - ⏱ 2021-02-06 01:39:10 
# 读书笔记

# 本书评论
