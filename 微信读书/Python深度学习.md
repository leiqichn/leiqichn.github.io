---
doc_type: weread-highlights-reviews
bookId: "26211849"
author: 弗朗索瓦·肖莱
cover: https://cdn.weread.qq.com/weread/cover/83/YueWen_26211849/t7_YueWen_26211849.jpg
reviewCount: 2
noteCount: 134
isbn: 9787115488763
category: 计算机-编程设计
lastReadDate: 2020-10-26
---
# 元数据
> [!abstract] Python深度学习
> - ![ Python深度学习|200](https://cdn.weread.qq.com/weread/cover/83/YueWen_26211849/t7_YueWen_26211849.jpg)
> - 书名： Python深度学习
> - 作者： 弗朗索瓦·肖莱
> - 简介： 本书由Keras之父、现任Google人工智能研究员的弗朗索瓦·肖莱（François Chollet）执笔，详尽介绍了用Python和Keras进行深度学习的探索实践，包括计算机视觉、自然语言处理、产生式模型等应用。书中包含30多个代码示例，步骤讲解详细透彻。由于本书立足于人工智能的可达性和大众化，读者无须具备机器学习相关背景知识即可展开阅读。在学习完本书后，读者将具备搭建自己的深度学习环境、建立图像识别模型、生成图像和文字等能力。
> - 出版时间 2018-08-24 00:00:00
> - ISBN： 9787115488763
> - 分类： 计算机-编程设计
> - 出版社： 人民邮电出版社

# 高亮划线

## 2.1 初识神经网络


- 📌 train_images和train_labels组成了训练集（training set），模型将从这些数据中进行学习。然后在测试集（test set，即test_images和test_labels）上对模型进行测试。 ^26211849-15-1826-1988
    - ⏱ 2020-08-25 01:03:31 

- 📌 大多数深度学习都是将简单的层链接起来，从而实现渐进式的数据蒸馏（data distillation）。 ^26211849-15-3297-3374
    - ⏱ 2020-08-25 01:05:39 

- 📌 本例中的网络包含2个Dense层，它们是密集连接（也叫全连接）的神经层。 ^26211849-15-3441-3503
    - ⏱ 2020-08-25 01:06:06 
## 2.2 神经网络的数据表示


- 📌 前面例子使用的数据存储在多维Numpy数组中，也叫张量（tensor） ^26211849-16-430-491
    - ⏱ 2020-08-25 01:10:25 

- 📌 ［注意，张量的维度（dimension）通常叫作轴（axis） ^26211849-16-673-756
    - ⏱ 2020-08-25 01:11:03 

- 📌 仅包含一个数字的张量叫作标量（scalar，也叫标量张量、零维张量、0D张量） ^26211849-16-857-922
    - ⏱ 2020-08-25 10:38:45 

- 📌 标量张量有0个轴（ndim==0）。张量轴的个数也叫作阶（rank ^26211849-16-995-1051
    - ⏱ 2020-08-25 10:39:12 

- 📌 不要把5D向量和5D张量弄混！5D向量只有一个轴，沿着轴有5个维度，而5D张量有5个轴（沿着每个轴可能有任意个维度） ^26211849-16-1690-1748
    - ⏱ 2020-08-25 10:40:57 

- 📌 5阶张量（张量的阶数即轴的个数 ^26211849-16-1867-1908
    - ⏱ 2020-08-25 10:42:05 

- 📌 深度学习处理的一般是0D到4D的张量，但处理视频数据时可能会遇到5D张量。 ^26211849-16-3308-3345
    - ⏱ 2020-08-25 10:44:36 

- 📌 这在Numpy等Python库中也叫张量的ndim。 ^26211849-16-3542-3568
    - ⏱ 2020-08-25 10:44:54 

- 📌 向量的形状只包含一个元素，比如(5, )，而标量的形状为空，即()。 ^26211849-16-3698-3732
    - ⏱ 2020-08-25 10:45:16 

- 📌 更确切地说，它是60000个矩阵组成的数组，每个矩阵由28×28个整数组成。每个这样的矩阵都是一张灰度图像，元素取值范围为0~255。 ^26211849-16-4631-4698
    - ⏱ 2020-08-25 10:48:47 

- 📌 我们使用语法train_images[i]来选择沿着第一个轴的特定数字。选择张量的特定元素叫作张量切片（tensor slicing ^26211849-16-5347-5459
    - ⏱ 2020-08-25 10:54:49 

- 📌 与Python列表中的负数索引类似，它表示与当前轴终点的相对位置。你可以在图像中心裁剪出14像素×14像素的区域： ^26211849-16-6164-6221
    - ⏱ 2020-08-25 10:56:38 

- 📌 第一个轴（0轴）叫作批量轴（batch axis）或批量维度（batchdimension）。在使用Keras和其他深度学习库时，你会经常遇到这个术语。 ^26211849-16-6931-7060
    - ⏱ 2020-08-25 10:58:13 

- 📌 当时间（或序列顺序）对于数据很重要时，应该将数据存储在带有时间轴的3D张量中。每个样本可以被编码为一个向量序列（即2D张量 ^26211849-16-8375-8436
    - ⏱ 2020-08-25 11:00:33 

- 📌 一个形状为(390, 3)的2D张量（一个交易日有390分钟），而250天的数据则可以保存在一个形状为(250, 390, 3)的3D张量中。这里每个样本是一天的股票数据。 ^26211849-16-8880-8966
    - ⏱ 2020-08-25 11:01:29 

- 📌 高度、宽度和颜色深度 ^26211849-16-9294-9304
    - ⏱ 2020-08-25 11:04:34 

- 📌 Keras框架同时支持这两种格式。 ^26211849-16-10120-10137
    - ⏱ 2020-08-25 11:05:43 
## 2.3 神经网络的“齿轮”：张量运算


- 📌 这些运算都是优化好的Numpy内置函数，这些函数将大量运算交给安装好的基础线性代数子程序（BLAS, basic linear algebra subprograms）实现（没装的话，应该装一个） ^26211849-17-2066-2164
    - ⏱ 2020-08-25 13:33:47 

- 📌 点积运算，也叫张量积（tensor product，不要与逐元素的乘积弄混），是最常见也最有用的张量运算。 ^26211849-17-3885-3964
    - ⏱ 2020-08-25 13:36:28 

- 📌 但在Numpy和Keras中，都是用标准的dot运算符来实现点 ^26211849-17-4085-4116
    - ⏱ 2020-08-25 13:36:49 

- 📌 注意，两个向量之间的点积是一个标量，而且只有元素个数相同的向量之间才能做点积。 ^26211849-17-4558-4597
    - ⏱ 2020-08-25 13:40:50 

- 📌 你还可以对一个矩阵x和一个向量y做点积，返回值是一个向量，其中每个元素是y和x的每一行之间的点积。其实现过程如下。 ^26211849-17-4626-4683
    - ⏱ 2020-08-25 13:41:10 

- 📌 张量变形是指改变张量的行和列，以得到想要的形状。变形后的张量的元素总个数与初始张量相同。 ^26211849-17-6602-6646
    - ⏱ 2020-08-25 13:47:19 

- 📌 经常遇到的一种特殊的张量变形是转置（transposition）。对矩阵做转置是指将行和列互换，使x[i, :]变为x[:, i]。 ^26211849-17-7226-7344
    - ⏱ 2020-08-25 13:48:13 
## 2.4 神经网络的“引擎”：基于梯度的优化


- 📌 一开始，这些权重矩阵取较小的随机值，这一步叫作随机初始化（randominitialization）。当然，W和b都是随机的，relu(dot(W, input)+b)肯定不会得到任何有用的表示。虽然得到的表示是没有意义的， ^26211849-18-755-894
    - ⏱ 2020-08-25 13:52:08 

- 📌 动量方法的实现过程是每一步都移动小球，不仅要考虑当前的斜率值（当前的加速度），还要考虑当前的速度（来自于之前的加速度）。这在实践中的是指，更新参数w不仅要考虑当前的梯度值，还要考虑上一次的参数更新，其简单实现如下所示。 ^26211849-18-7916-8025
    - ⏱ 2020-08-25 14:16:14 
## 2.5 回顾第一个例子


- 📌 在所有训练数据上迭代一次叫作一个轮次（epoch ^26211849-19-1943-1989
    - ⏱ 2020-08-25 21:04:55 

- 📌 在每次迭代过程中，网络会计算批量损失相对于权重的梯度，并相应地更新权重 ^26211849-19-1996-2031
    - ⏱ 2020-08-25 21:05:08 
## 本章小结


- 📌 个关键的概念：损失和优化器。将数据输入网络之前，你需要先定义这二者。 ^26211849-20-753-839
    - ⏱ 2020-08-25 21:05:40 

- 📌 损失是在训练过程中需要最小化的量，因此，它应该能够衡量当前任务是否已成功解决。 ^26211849-20-890-936
    - ⏱ 2020-08-25 21:05:48 
## 3.1 神经网络剖析


- 📌 例如，简单的向量数据保存在形状为(samples, features)的2D张量中，通常用密集连接层［densely connected layer，也叫全连接层（fully connected layer）或密集层（dense layer），对应于Keras的Dense类］来处理 ^26211849-22-1616-1835
    - ⏱ 2020-08-28 00:43:18 

- 📌 序列数据保存在形状为(samples, timesteps, features)的3D张量中，通常用循环层（recurrent layer，比如Keras的LSTM层）来处理 ^26211849-22-1836-1949
    - ⏱ 2020-08-28 00:43:39 

- 📌 图像数据保存在4D张量中，通常用二维卷积层（Keras的Conv2D）来处理。 ^26211849-22-1950-1989
    - ⏱ 2020-08-28 00:43:50 

- 📌 这里层兼容性（layer compatibility）具体指的是每一层只接受特定形状的输入张量，并返回特定形 ^26211849-22-2099-2179
    - ⏱ 2020-08-28 00:44:13 

- 📌 选择正确的目标函数对解决问题是非常重要的 ^26211849-22-4187-4207
    - ⏱ 2020-08-28 00:50:41 

- 📌 例如，对于二分类问题，你可以使用二元交叉熵（binary crossentropy）损失函数；对于多分类问题，可以用分类交叉熵（categorical crossentropy）损失函数；对于回归问题，可以用均方误差（mean-squared error）损失函数；对于序列学习问题，可以用联结主义时序分类（CTC, connectionist temporal classification）损失函数 ^26211849-22-4550-4751
    - ⏱ 2020-08-28 00:51:50 
## 3.2 Keras简介


- 📌 相反，它依赖于一个专门的、高度优化的张量库来完成这些运算，这个张量库就是Keras的后端引擎（backend engine） ^26211849-23-1631-1719
    - ⏱ 2020-08-28 00:54:14 

- 📌 配置学习过程是在编译这一步，你需要指定模型使用的优化器和损失函数，以及训练过程中想要监控的指标 ^26211849-23-4171-4218
    - ⏱ 2020-08-28 00:58:02 

- 📌 ，学习过程就是通过fit()方法将输入数据的Numpy数组（和对应的目标数据）传入模型，这一做法与Scikit-Learn及其他机器学习库类似。 ^26211849-23-4475-4547
    - ⏱ 2020-08-28 00:58:36 
## 3.4 电影评论分类：二分类问题


- 📌 中间层使用relu作为激活函数，最后一层使用sigmoid激活以输出一个0~1范围内的概率值（表示样本的目标值等于1的可能性，即评论为正面的可能性） ^26211849-25-4276-4350
    - ⏱ 2020-08-28 01:11:19 

- 📌 （crossentropy）往往是最好的选择。交叉熵是来自于信息论领域的概念，用于衡量概率分布之间的距离 ^26211849-25-6424-6476
    - ⏱ 2020-08-28 01:14:09 

- 📌 下面的步骤是用rmsprop优化器和binary_crossentropy损失函数来配置模型。注意，我们还在训练过程中监控精度。代码清单3-4 编译模型model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])上述代码将优化器、损失函数和指标作为字符串传入，这是因为rmsprop、binary_crossentropy和accuracy都是Keras内置的一部分。有时你可能希望配置自定义优化器的参数，或者传入自定义的损失函数或指标函数。前者可通过向optimizer参数传入一个优化器类实例来实现，如代码清单3-5所示；后者可通过向loss和metrics参数传入函数对象来实现，如代码清单3-6所示。代码清单3-5 配置优化器 ^26211849-25-6483
    - ⏱ 2020-08-28 01:14:14 

- 📌 两个隐藏层 ^26211849-25-11949-11954
    - ⏱ 2020-08-28 08:16:18 

- 📌 mse损失函数代替binary_crossentropy。 ^26211849-25-12083-12112
    - ⏱ 2020-08-28 08:16:30 

- 📌 尝试使用tanh激活（这种激活在神经网络早期非常流行）代替relu。 ^26211849-25-12144-12178
    - ⏱ 2020-08-28 08:16:38 

- 📌 对于二分类问题的sigmoid标量输出，你应该使用binary_crossentropy损失函数。 ^26211849-25-12601-12650
    - ⏱ 2020-08-28 08:29:04 
## 3.5 新闻分类：多分类问题


- 📌 前面提到，最终输出是46维的，因此中间层的隐藏单元个数不应该比46小太多。现在来看一下，如果中间层的维度远远小于46（比如4维），造成了信息瓶颈，那么会发生什么？ ^26211849-26-9573-9654
    - ⏱ 2020-08-28 11:34:48 
## 3.6 预测房价：回归问题


- 📌 令人困惑的是，logistic回归不是回归算法，而是分类算法。 ^26211849-27-732-763
    - ⏱ 2020-08-29 00:24:55 
## 本章小结


- 📌 如果数据被分为多个类别，那么中间层过小可能会导致信息瓶颈。 ^26211849-28-789-818
    - ⏱ 2020-08-29 00:39:01 
## 4.2 评估机器学习模型


- 📌 先来介绍三种经典的评估方法：简单的留出验证、K折验证，以及带有打乱数据的重复K折验证。 ^26211849-31-1931-2030
    - ⏱ 2020-08-29 19:38:26 

- 📌 具体做法是多次使用K折验证，在每次将数据划分为K个分区 ^26211849-31-4229-4312
    - ⏱ 2020-08-29 19:45:52 
## 4.3 数据预处理、特征工程和特征学习


- 📌 为了让网络的学习变得更容易，输入数据应该具有以下特征。❑ 取值较小：大部分值都应该在0~1范围内。❑ 同质性（homogenous）：所有特征的取值都应该在大致相同的范围内。 ^26211849-32-1441-1640
    - ⏱ 2020-08-29 19:50:41 

- 📌 这就是特征工程的本质：用更简单的方式表述问题，从而使问题变得更容易。它通常需要深入理解问题。 ^26211849-32-3628-3674
    - ⏱ 2020-08-30 00:19:48 
## 4.4 过拟合与欠拟合


- 📌 始终牢记：深度学习模型通常都很擅长拟合训练数据，但真正的挑战在于泛化，而不是拟合。 ^26211849-33-1854-1895
    - ⏱ 2020-08-30 00:24:35 

- 📌 网络的容量越大，它拟合训练数据（即得到很小的训练损失）的速度就越快，但也更容易过拟 ^26211849-33-4344-4385
    - ⏱ 2020-08-30 00:27:17 

- 📌 种常见的降低过拟合的方法就是强制让模型权重只能取较小的值，从而限制模型的复杂度，这使得权重值的分布更加规则（regular） ^26211849-33-5066-5154
    - ⏱ 2020-09-20 16:05:01 

- 📌 用他自己的话来说：“我去银行办理业务。柜员不停地换人，于是我问其中一人这是为什么。他说他不知道，但他们经常换来换去。我猜想，银行工作人员要想成功欺诈银行，他们之间要互相合作才行。这让我意识到，在每个样本中随机删除不同的部分神经元，可以阻止它们的阴谋，因此可以降低过拟合。 ^26211849-33-8833-8968
    - ⏱ 2020-08-30 14:39:39 

- 📌 防止神经网络过拟合的常用方法包括：❑ 获取更多的训练数据❑ 减小网络容量❑ 添加权重正则化❑ 添加dropout ^26211849-33-10177-10347
    - ⏱ 2020-08-30 14:40:34 
## 4.5 机器学习的通用工作流程


- 📌 你的输入数据是什么？你要预测什么？只有拥有可用的训练数据，你才能学习预测某件事情。 ^26211849-34-644-685
    - ⏱ 2020-08-30 14:42:34 

- 📌 面对的是什么类型的问题？是二分类问题、多分类问题、标量回归问题、向量回归问题，还是多分类、多标签问题？ ^26211849-34-793-844
    - ⏱ 2020-08-30 14:42:39 

- 📌 对于排序问题或多标签分类，你可以使用平均准确率均值（mean average precision） ^26211849-34-2193-2242
    - ⏱ 2020-08-30 14:50:45 

- 📌 。因此在分类任务中，常见的做法是优化ROC AUC的替代指标，比如交叉熵。一般来说，你可以认为交叉熵越小，ROC AUC越大。 ^26211849-34-4507-4570
    - ⏱ 2020-08-30 18:07:22 
## 5.1 卷积神经网络简介


- 📌 然后每个3D图块与学到的同一个权重矩阵［叫作卷积核（convolution kernel）］做张量积，转换成形状为(output_depth, )的1D向量。 ^26211849-38-9156-9261
    - ⏱ 2020-08-31 00:12:56 

- 📌 最大池化是从输入特征图中提取窗口，并输出每个通道的最大值。它的概念与卷积类似，但是最大池化使用硬编码的max张量运算对局部图块进行变换，而不是使用学到的线性变换（卷积核 ^26211849-38-12099-12183
    - ⏱ 2020-08-31 13:37:45 
## 5.2 在小型数据集上从头开始训练一个卷积神经网络


- 📌 然后，我们会介绍数据增强（data augmentation），它在计算机视觉领域是一种非常强大的降低过拟合的技术。使用数据增强之后，网络精度将提高到82%。 ^26211849-39-766-871
    - ⏱ 2020-08-31 21:30:14 

- 📌 用预训练的网络做特征提取（得到的精度范围在90%~96%），对预训练的网络进行微调（最终精度为97%） ^26211849-39-949-1033
    - ⏱ 2020-08-31 21:30:32 

- 📌 深度学习的一个基本特性就是能够独立地在训练数据中找到有趣的特征，无须人为的特征工程，而这只在拥有大量训练样本时才能实现。对于输入样本的维度非常高（比如图像）的问题尤其如此。 ^26211849-39-1256-1342
    - ⏱ 2020-08-31 21:31:48 

- 📌 但如果模型很小，并做了很好的正则化，同时任务非常简单，那么几百个样本可能就足够了。 ^26211849-39-1443-1484
    - ⏱ 2020-08-31 21:32:10 

- 📌 现在我们将使用一种针对于计算机视觉领域的新方法，在用深度学习模型处理图像时几乎都会用到这种方法，它就是数据增强（data augmentation）。 ^26211849-39-12826-12927
    - ⏱ 2020-08-31 22:01:32 
## 5.3 使用预训练的卷积神经网络


- 📌 这是因为本方法没有使用数据增强，而数据增强对防止小型图像数据集的过拟合非常重要。 ^26211849-40-10848-10888
    - ⏱ 2020-09-01 00:25:42 

- 📌 在Keras中，冻结网络的方法是将其trainable属性设为False ^26211849-40-13003-13039
    - ⏱ 2020-09-01 00:26:33 

- 📌 在小型数据集上的主要问题是过拟合。在处理图像数据时，数据增强是一种降低过拟合的强大方法 ^26211849-40-23650-23693
    - ⏱ 2020-09-01 00:33:09 
## 本章小结


- 📌 卷积神经网络学到的表示很容易可视化，卷积神经网络不是黑盒。 ^26211849-42-539-568
    - ⏱ 2020-09-01 01:09:44 
## 第6章 深度学习用于文本和序列


- 📌 用于处理序列的两种基本的深度学习算法分别是循环神经网络（recurrent neural network）和一维卷积神经网络（1D convnet ^26211849-43-738-857
    - ⏱ 2020-09-01 09:51:04 
## 6.1 处理文本数据


- 📌 ，深度学习模型不会接收原始文本作为输入，它只能处理数值张量。文本向量化（vectorize）是指将文本转换为数值张量的过程 ^26211849-44-705-792
    - ⏱ 2020-08-27 14:51:41 

- 📌 但从更实际的角度来说，一个好的词嵌入空间在很大程度上取决于你的任务 ^26211849-44-7727-7760
    - ⏱ 2020-08-27 16:27:00 

- 📌 将整数索引（表示特定单词）映射为密集向量。 ^26211849-44-8267-8288
    - ⏱ 2020-08-27 16:27:41 

- 📌 所以较短的序列应该用0填充，较长的序列应该被截断。 ^26211849-44-8830-8855
    - ⏱ 2020-08-27 16:28:46 

- 📌 然后可以用RNN层或一维卷积层来处理这个三维张量（二者都会在后面介绍）。 ^26211849-44-8964-9000
    - ⏱ 2020-08-27 14:58:04 

- 📌 你的结果可能会有所不同。训练样本数太少，所以模型性能严重依赖于你选择的200个样本，而样本是随机选择的。如果你得到的结果很差，可以尝试重新选择200个不同的随机样本，你可以将其作为练习 ^26211849-44-17598-17690
    - ⏱ 2020-08-27 16:43:18 

- 📌 预训练词嵌入 ^26211849-44-20655-20661
    - ⏱ 2020-09-29 09:46:14 
## 6.2 理解循环神经网络


- 📌 将全部电影评论转换 ^26211849-45-579-588
    - ⏱ 2020-08-27 17:00:09 

- 📌 生物智能以渐进的方式处理信息，同时保存一个关于所处理内容的内部模型，这个模型是根据过去的信息构建的，并随着新信息的进入而不断更新。 ^26211849-45-768-833
    - ⏱ 2020-08-27 17:00:44 

- 📌 总之，RNN是一个for循环，它重复使用循环前一次迭代的计算结果，仅此而已 ^26211849-45-2764-2801
    - ⏱ 2020-10-26 15:09:31 

- 📌 输入只考虑了前500个单词，而不是整个序列， ^26211849-45-8815-8837
    - ⏱ 2020-10-26 15:29:23 

- 📌 另一部分原因在于，SimpleRNN不擅长处理长序列，比如文本。 ^26211849-45-8859-8891
    - ⏱ 2020-10-26 15:29:32 

- 📌 其原因在于梯度消失问题（vanishing gradient problem），这一效应类似于在层数较多的非循环网络（即前馈网络）中观察到的效应：随着层数的增加，网络最终变得无法训练 ^26211849-45-9200-9317
    - ⏱ 2020-09-01 23:22:29 

- 📌 先来看LSTM层。其背后的长短期记忆（LSTM, long short-term memory）算法由Hochreiter和Schmidhuber在1997年开 ^26211849-45-9651-9731
    - ⏱ 2020-09-01 23:22:42 

- 📌 这实际上就是LSTM的原理：它保存信息以便后面使用，从而防止较早期的信号在处理过程中逐渐消失。 ^26211849-45-10057-10104
    - ⏱ 2020-08-27 17:18:19 

- 📌 它在不同的时间步的值叫作Ct，其中C表示携带（carry） ^26211849-45-10563-10618
    - ⏱ 2020-10-26 15:37:33 

- 📌 一个原因是你没有花力气来调节超参数，比如嵌入维度或LSTM输出维度。另一个原因可能是缺少正则化 ^26211849-45-14392-14439
    - ⏱ 2020-10-26 16:07:05 

- 📌 对于这样的基本问题，观察每条评论中出现了哪些词及其出现频率就可以很好地解决。这也正是第一个全连接方法的做法。但还有更加困难的自然语言处理问题，特别是问答和机器翻译，这时LSTM的优势就明显了。 ^26211849-45-14493-14589
    - ⏱ 2020-10-26 16:07:27 
## 6.3 循环神经网络的高级用法


- 📌 永远要记住，面对市场时，过去的表现并不能很好地预测未来的收益，正如靠观察后视镜是没办法开车的。与此相对的是，如果在数据集中过去能够很好地预测未来，那么机 ^26211849-46-23109-23211
    - ⏱ 2020-08-27 17:30:35 
## 6.4 用卷积神经网络处理序列


- 📌 这些性质让卷积神经网络在计算机视觉领域表现优异，同样也让它对序列处理特别有效。时间可以被看作一个空间维度，就像二维图像的高度或宽度。 ^26211849-47-551-617
    - ⏱ 2020-08-27 17:33:51 

- 📌 这种一维卷积神经网络的效果可以媲美RNN ^26211849-47-657-675
    - ⏱ 2020-08-27 17:33:58 

- 📌 一维卷积层可以识别序列中的局部模式 ^26211849-47-1316-1333
    - ⏱ 2020-08-27 17:36:49 

- 📌 这一点与RNN不同。当然，为了识别更长期的模式，你可以将许多卷积层和池化层堆叠在一起，这样上面的层能够观察到原始输入中更长的序列段 ^26211849-47-4983-5048
    - ⏱ 2020-09-29 11:52:59 

- 📌 。对于那些非常长，以至于RNN无法处理的序列（比如包含上千个时间步的序列），这种方法尤其有用。卷积神经网络可以将长的输入序列转换为高级特征组成的更短序列（下采样） ^26211849-47-6881-6962
    - ⏱ 2020-09-29 14:32:02 

- 📌 因为RNN在处理非常长的序列时计算代价很大，但一维卷积神经网络的计算代价很小，所以在RNN之前使用一维卷积神经网络作为预处理步骤是一个好主意，这样可以使序列变短，并提取出有用的表示交给RNN来处理。 ^26211849-47-9710-9809
    - ⏱ 2020-08-28 00:36:16 
## 7.1 不用Sequential模型的解决方案：Keras函数式API


- 📌 。这种1×1卷积［也叫作逐点卷积（pointwise convolu ^26211849-50-10935-10995
    - ⏱ 2020-09-03 01:00:28 

- 📌 残差连接解决了困扰所有大规模深度学习模型的两个共性问题：梯度消失和表示瓶颈。 ^26211849-50-12706-12744
    - ⏱ 2020-09-03 01:04:17 

- 📌 那么下游操作将永远无法恢复那些被丢弃的频段。任何信息的丢失都是永久性的。残差连接可以将较早的信息重新注入到下游数据中，从而部分解决了深度学习模型的这一问题。 ^26211849-50-14067-14145
    - ⏱ 2020-09-03 01:07:12 
## 7.2 使用Keras回调函数和TensorBoard来检查并监控深度学习模型


- 📌 。前面所有例子都采用这样一种策略：训练足够多的轮次，这时模型已经开始过拟合，根据这第一次运行来确定训练所需要的正确轮数，然后使用这个最佳轮数从头开始再启动一次新的训练。当然，这种方法很浪费 ^26211849-51-867-961
    - ⏱ 2020-09-03 01:13:38 

- 📌 。回调函数（callback）是在调用fit时传入模型的一个对象（即实现特定方法的类实例），它在训练过程中的不同时间点都会被模型调用。它可以访问关于模型状态与性能的所有可用数据，还可以采取行动：中断训练、保存模型、加载一组不同的权重或改变模型的状态。 ^26211849-51-1040-1191
    - ⏱ 2020-09-03 01:14:15 

- 📌 从可视化图中可以立刻明显地看出，将嵌入与特定目标联合训练得到的模型是完全针对这个特定任务的，这也是为什么使用预训练的通用词嵌入通常不是一个好主意。 ^26211849-51-7291-7364
    - ⏱ 2020-09-03 01:20:37 

- 📌 使用这个函数需要安装Python的pydot库和pydot-ng库，还需要安装graphviz库。我们来快速看一下。 ^26211849-51-8288-8346
    - ⏱ 2020-09-03 01:21:34 
## 7.3 让模型性能发挥到极致


- 📌 1．批标准化标准化（normalization）是一大类方法，用于让机器学习模型看到的不同样本彼此之间更加相似， ^26211849-52-769-880
    - ⏱ 2020-09-03 01:22:30 

- 📌 。批标准化的工作原理是，训练过程中在内部保存已读取每批数据均值和方差的指数移动平均值。 ^26211849-52-1671-1714
    - ⏱ 2020-09-03 11:59:55 

- 📌 还有很多。这些在架构层面的参数叫作超参数（hyperparameter），以便将其与模型参数区分开来，后者通过反向传播进行训练。 ^26211849-52-5580-5670
    - ⏱ 2020-09-04 10:20:47 

- 📌 超参数优化是一项强大的技术，想要在任何任务上获得最先进的模型或者赢得机器学习竞赛，这项技术都必不可少 ^26211849-52-7327-7377
    - ⏱ 2020-09-04 10:24:32 

- 📌 通常来说，更好的分类器被赋予更大的权重，而较差的分类器则被赋予较小的权重。为了找到一组好的集成权重，你可以使用随机搜索或简单的优化算法（比如Nelder-Mead方法） ^26211849-52-8764-8848
    - ⏱ 2020-09-04 10:26:05 

- 📌 用机器学习的术语来说，如果所有模型的偏差都在同一个方向上，那么集成也会保留同样的偏差。如果各个模型的偏差在不同方向上，那么这些偏差会彼此抵消，集成结果会更加稳定、更加准确。 ^26211849-52-9298-9410
    - ⏱ 2020-09-04 10:26:39 

- 📌 我发现有一种方法在实践中非常有效（但这一方法还没有推广到所有问题领域），就是将基于树的方法（比如随机森林或梯度提升树）和深度神经网络进行集成 ^26211849-52-9702-9772
    - ⏱ 2020-09-04 10:32:46 

- 📌 一种在实践中非常成功的基本集成方法是宽且深（wide and deep）的模型类型，它结合了深度学习与浅层学习。这种模型联合训练一个深度神经网络和一个大型的线性模型。对多种模型联合训练，是实现模型集成的另一种选择。 ^26211849-52-10052-10185
    - ⏱ 2020-09-04 10:33:47 

- 📌 构建高性能的深度卷积神经网络时，你需要使用残差连接、批标准化和深度可分离卷积。未来，无论是一维、二维还是三维应用，深度可分离卷积很可能会完全取代普通卷积，因为它的表示效率更高。 ^26211849-52-10281-10369
    - ⏱ 2020-09-04 10:34:12 

- 📌 但Hyperopt和Hyperas这两个库可能会对你有所帮助。进行超参数优化时，一定要小 ^26211849-52-10505-10549
    - ⏱ 2020-09-04 10:34:34 

- 📌 多样性就是力量。将非常相似的模型集成基本上是没有意义的。 ^26211849-52-10671-10699
    - ⏱ 2020-09-04 10:34:45 
## 本章总结


- 📌 TensorBoard可以将指标、激活直方图甚至嵌入空间可视化。 ^26211849-53-619-651
    - ⏱ 2020-09-04 10:35:04 
## 8.5 生成式对抗网络简介


- 📌 它可以替代VAE来学习图像的潜在空间。它能够迫使生成图像与真实图像在统计上几乎无法区分，从而生成相当逼真的合成图像。 ^26211849-59-648-706
    - ⏱ 2020-09-05 13:10:07 

- 📌 ：一个伪造者网络和一个专家网络，二者训练的目的都是为了打败彼此。因此，GAN由以下两部分组成。 ^26211849-59-995-1042
    - ⏱ 2020-09-05 13:10:48 

- 📌 生成器网络（generator network）：它以一个随机向量（潜在空间中的一个随机点）作为输入，并将其解码为一张合成图像。❑ 判别器网络（discriminator network）或对手（adversary）：以一张图像（真实的或合成的均可）作为输入，并预测该图像是来自训练集还是由生成器网络创建。 ^26211849-59-1073-1335
    - ⏱ 2020-09-05 13:10:57 
## 9.1 重点内容回顾


- 📌 直观上来看，这意味着从输入到输出的几何变形必须是平滑且连续的，这是一个很重要的约束条件。 ^26211849-62-3185-3229
    - ⏱ 2020-09-01 23:31:05 

- 📌 它包括CUDA语言、像TensorFlow这样能够做自动求微分的框架和Keras, Keras让大多数人都可以使用深度学习。 ^26211849-62-4876-4938
    - ⏱ 2020-09-01 23:33:41 

- 📌 ：之所以叫作密集连接，是因为Dense层的每个单元都和其他所有单元相连接。 ^26211849-62-7524-7587
    - ⏱ 2020-09-01 23:38:25 

- 📌 请记住：对于二分类问题（binary classification），层堆叠的最后一层是使用sigmoid激活且只有一个单元的Dense层，并使用binary_crossentropy作为损失。目标应该是0或1。 ^26211849-62-7822-7954
    - ⏱ 2020-09-01 23:38:44 

- 📌 对于多标签多分类问题（multilabel categorical classification，每个样本可以有多个类别），层堆叠的最后一层是一个Dense层，它使用sigmoid激活，其单元个数等于类别个数，并使用binary_crossentropy作为损失。目标应该是k-hot编码的。 ^26211849-62-9003-9203
    - ⏱ 2020-09-01 23:39:33 
## 9.2 深度学习的局限性


- 📌 但人类不需要死亡就可以学会安全行为，这也要归功于我们对假想情景进行抽象建模的能力 ^26211849-63-4428-4468
    - ⏱ 2020-09-01 23:50:58 
## 9.3 深度学习的未来


- 📌 当前模型的致命弱点正是缺少推理和 ^26211849-64-818-834
    - ⏱ 2020-09-01 23:52:56 

- 📌 这通常使用Hyperopt等库来实现，我们在第7章介绍过。但我们还可以更有野心，尝试从头开始学习合适的架构，让约束尽可能少，比如可以通过强化学习或遗传算法来实现。 ^26211849-64-4597-4678
    - ⏱ 2020-09-02 15:26:01 

- 📌 在训练数据上进行反向传播来调节模型特征的同时，还能够不断调节其模型架构。在我写到本节内容时，这种方法已经开始出现了。 ^26211849-64-4794-4852
    - ⏱ 2020-09-02 15:26:39 

- 📌 近年来有一个反复出现的观察结果值得注意：训练同一个模型同时完成几个几乎没有联系的任务，这样得到的模型在每个任务上的效果都更好。 ^26211849-64-5379-5494
    - ⏱ 2020-09-02 15:28:20 
## 9.4 了解一个快速发展领域的最新进展


- 📌 大部分竞赛的获胜者都使用XGBoost库（用于浅层机器学习）或Keras（用于深度学习）。 ^26211849-65-1023-1068
    - ⏱ 2020-09-02 15:30:18 
# 读书笔记

## 2.3 神经网络的“齿轮”：张量运算

### 划线评论
- 📌 数学符号中的点（.）表示  ^11897562-7jRz8yaOx
    - 💭 点积就是相同位置的数字相乘

    - ⏱ 2020-08-25 13:40:20
   
## 3.1 神经网络剖析

### 划线评论
- 📌 这个层将返回一个张量，第一个维度的大小变成了32。  ^11897562-7jVjnhkkj
    - 💭 神经元降维
    - ⏱ 2020-08-28 00:46:20
   
# 本书评论
