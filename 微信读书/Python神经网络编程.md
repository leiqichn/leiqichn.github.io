---
doc_type: weread-highlights-reviews
bookId: "22655611"
author: 塔里克·拉希德
cover: https://cdn.weread.qq.com/weread/cover/56/YueWen_22655611/t7_YueWen_22655611.jpg
reviewCount: 0
noteCount: 69
isbn: 9787115474810
category: 计算机-人工智能
lastReadDate: 2020-04-13
---
# 元数据
> [!abstract] Python神经网络编程
> - ![ Python神经网络编程|200](https://cdn.weread.qq.com/weread/cover/56/YueWen_22655611/t7_YueWen_22655611.jpg)
> - 书名： Python神经网络编程
> - 作者： 塔里克·拉希德
> - 简介： 本书首先从简单的思路着手，详细介绍了理解神经网络如何工作所必须的基础知识。第一部分介绍基本的思路，包括神经网络底层的数学知识，第2部分是实践，介绍了学习Python编程的流行和轻松的方法，从而逐渐使用该语言构建神经网络，以能够识别人类手写的字母，特别是让其像专家所开发的网络那样地工作。第3部分是扩展，介绍如何将神经网络的性能提升到工业应用的层级，甚至让其在RaspberryPi上工作。
> - 出版时间 2018-04-01 00:00:00
> - ISBN： 9787115474810
> - 分类： 计算机-人工智能
> - 出版社： 人民邮电出版社

# 高亮划线

## 内容提要


- 📌 。第1章介绍了神经网络中所用到的数学思想。第2章介绍使用Python实现神经网络，识别手写数字，并测试神经网络的性能。第3章带领读者进一步了解简单的神经网络，观察已受训练的神经网络内部，尝试进一步改善神经网络的性能，并加深对相关知识的理解 ^22655611-3-536-655
    - ⏱ 2020-04-12 09:09:38 
## 序言


- 📌 如对相似的照片进行分组、从健康细胞中识别出病变细胞，甚至是来一盘优雅的国际象棋博弈 ^22655611-5-633-674
    - ⏱ 2020-04-12 09:54:23 

- 📌 如让计算机学习如何玩视频游戏 ^22655611-5-2289-2303
    - ⏱ 2020-04-12 09:56:45 

- 📌 让你了解神经网络如何工作，帮你制作出自己的神经网络，训练神经网络来识别人类的手写字符 ^22655611-5-2587-2629
    - ⏱ 2020-04-12 09:57:12 
## 前言


- 📌 任何希望了解什么是神经网络的读者而编写的，是为了任何希望设计和使用自己神经网络的读者而编写的，也是为了任何希望领略那些在神经网络发挥核心作用、相对容易但激动人心的数学思想的读者而编写的。 ^22655611-6-470-563
    - ⏱ 2020-04-12 09:57:28 

- 📌 优雅从容地解释神经网络，解释神经网络的实现，激起学生对神经网络的热情，鼓励学生使用短短的几行代码制作出能够学习的人工智能。 ^22655611-6-907-968
    - ⏱ 2020-04-12 09:58:40 

- 📌 非常简单的预测神经元开始，然后逐步改进它们，直到达到它们的极限。 ^22655611-6-1395-1427
    - ⏱ 2020-04-12 09:59:23 

- 📌 一览在简单的神经网络中所用的数学思想。我们有意不介绍任何计算机编程知识，以避免喧宾夺主地干扰了本书的核心思想。 ^22655611-6-2423-2478
    - ⏱ 2020-04-12 10:00:15 

- 📌 我们将学习足以实现自己的神经网络的Python知识。我们将训练神经网络，识别手写数字，并且会测试神经网络的性能。 ^22655611-6-2515-2571
    - ⏱ 2020-04-12 10:00:27 
## 1.1 尺有所短，寸有所长


- 📌 有些任务，对传统的计算机而言很容易，对人类而言却很难。例如，对数百万个数字进行乘法运算。另一方面，有些任务对传统的计算机而言很难，对人类而言却很容易。例如，从一群人的照片中识别出面孔。 ^22655611-8-1919-2047
    - ⏱ 2020-04-12 10:03:15 
## 1.2 一台简单的预测机


- 📌 我们不必尝试使用代数法计算出C需要改变的确切量，让我们继续使用这种方法改进C值。 ^22655611-9-3736-3776
    - ⏱ 2020-04-12 10:05:41 

- 📌 请记住上面的公式，误差值等于真实值减去计算值。 ^22655611-9-4209-4232
    - ⏱ 2020-04-12 10:06:25 

- 📌 ，应该适度调整C值。如果输出值越来越接近正确答案，即误差值越来越小，那么我们就不要做那么大的调整。 ^22655611-9-4646-4695
    - ⏱ 2020-04-12 10:06:46 

- 📌 大误差意味着需要大的修正值，小误差意味着我们只需要小小地微调C的值。 ^22655611-9-4822-4856
    - ⏱ 2020-04-12 10:07:05 

- 📌 就是走马观花地浏览了一遍神经网络中学习的核心过程。我们训练机器，使其输出值越来越接近正确的答案。 ^22655611-9-4901-4949
    - ⏱ 2020-04-12 10:07:15 

- 📌 我们尝试得到一个答案，并多次改进答案，这是一种非常不同的方法。一些人将这种方法称为迭代，意思是持续地、一点一点地改进答案 ^22655611-9-5035-5095
    - ⏱ 2020-04-12 10:07:27 
## 1.3 分类器与预测器并无太大差别


- 📌 并做出应有的预测，输出结果，所以我们将其称为预测器 ^22655611-10-492-517
    - ⏱ 2020-04-12 10:08:04 

- 📌 我们根据结果与已知真实示例进行比较所得到的误差，调整内部参数，使预测更加精确 ^22655611-10-518-556
    - ⏱ 2020-04-12 10:08:20 
## 1.4 训练简单的分类器


- 📌 一台分类器。 ^22655611-11-1730-1736
    - ⏱ 2020-04-12 10:13:10 

- 📌 我们希望能够找到一种可重复的方法，也就是用一系列的计算机指令来达到这个目标。计算机科学家称这一系列指令为算法（algorithm）。 ^22655611-11-2601-2667
    - ⏱ 2020-04-12 10:13:59 

- 📌 也就是说，E等于t-y。 ^22655611-11-5090-5158
    - ⏱ 2020-04-12 10:19:54 

- 📌 最终改进的直线与最后一次训练样本非常匹配。 ^22655611-11-7850-7871
    - ⏱ 2020-04-12 10:25:43 

- 📌 最终改进的直线不会顾及所有先前的训练样本，而是抛弃了所有先前训练样本的学习结果，只是对最近的一个实例进行了学习。 ^22655611-11-7875-7931
    - ⏱ 2020-04-12 10:25:52 

- 📌 进行适度改进（moderate ^22655611-11-8026-8034
    - ⏱ 2020-04-12 10:26:00 

- 📌 。我们采用ΔA几分之一的一个变化值，而不是采用整个ΔA，充满激情地跳跃到每一个新的A值。 ^22655611-11-8059-8103
    - ⏱ 2020-04-12 10:26:48 

- 📌 我们小心翼翼地调整参数C，使其只是实际误差值的几分之几。 ^22655611-11-8190-8218
    - ⏱ 2020-04-12 10:27:05 

- 📌 但是这一次，在改进公式中，我们将添加一个调节系数 ^22655611-11-8409-8433
    - ⏱ 2020-04-12 10:28:11 

- 📌 学习率（learning rate） ^22655611-11-8544-8562
    - ⏱ 2020-04-12 10:28:17 

- 📌 适度更新有助于限制这些错误样本的影响。 ^22655611-11-10275-10294
    - ⏱ 2020-04-12 10:30:09 
## 1.5 有时候一个分类器不足以求解问题


- 📌 却不足以求解一些更有趣的问题，而我们希望应用神经网络来求解这些问题。 ^22655611-12-563-597
    - ⏱ 2020-04-12 10:30:41 

- 📌 我们为什么要说明线性分类器的局限性， ^22655611-12-658-676
    - ⏱ 2020-04-12 10:30:59 

- 📌 这种函数只有在A或B仅有一个为真但两个输入不同时为真的情况下，才输出为真 ^22655611-12-3184-3220
    - ⏱ 2020-04-12 10:41:00 
## 1.6 神经元——大自然的计算机器


- 📌 虽然一些计算机拥有大量的电子计算元件、巨大的存储空间，并且这些计算机的运行频率比肉蓬蓬、软绵绵的生物大脑要快得多，但是即使是像鸽子一样小的大脑，其能力也远远大于这些电子计算机，这使得科学家们对动物的大脑迷惑不解。 ^22655611-13-484-590
    - ⏱ 2020-04-12 10:43:37 

- 📌 有许多激活函数可以达到这样的效果。一个简单的阶跃函数可以实现这种效果。 ^22655611-13-2529-2564
    - ⏱ 2020-04-12 10:52:10 

- 📌 有时也称为逻辑函数 ^22655611-13-3307-3316
    - ⏱ 2020-04-12 10:55:48 

- 📌 超越数 ^22655611-13-3618-3621
    - ⏱ 2020-04-12 10:56:08 

- 📌 在某种意义上有点模糊的计算。 ^22655611-13-5008-5022
    - ⏱ 2020-04-12 11:01:04 

- 📌 ，每个神经元接受来自其之前多个神经元的输入 ^22655611-13-5348-5369
    - ⏱ 2020-04-12 11:01:42 

- 📌 每一层中的神经元都与在其前后层的神经元互相连接。下图详细描述了这种思想。 ^22655611-13-5456-5492
    - ⏱ 2020-04-12 11:02:06 
## 1.7 在神经网络中追踪信号


- 📌 从输入变成输出，这个过程似乎有点令人生畏，这好像是一种非常艰苦的工作。 ^22655611-14-558-593
    - ⏱ 2020-04-12 11:07:14 
## 1.8 凭心而论，矩阵乘法大有用途


- 📌 那么，矩阵如何帮助我们简化计算呢？其实，矩阵在两个方面帮助了我们。 ^22655611-15-769-802
    - ⏱ 2020-04-12 11:20:16 

- 📌 第二个好处是，许多计算机编程语言理解如何与矩阵一起工作，计算机编程语言能够认识到实际的工作是重复性的，因此能够高效高速地进行计算。 ^22655611-15-875-940
    - ⏱ 2020-04-12 11:20:33 

- 📌 矩阵的元素也不必是数字，它们也可以是我们命名的、但还未赋予实际的数值的一个量。因此，以下是这样一个矩阵：每个元素都是一个变量，具有一定的意义。虽然每个元素也可以具有一个数字数值，但是我们只是还未说明这些数值为多少。 ^22655611-15-1834-1941
    - ⏱ 2020-04-12 11:21:42 

- 📌 你会看到这样的矩阵乘法称为点乘（dot product）或内积（innerproduct） ^22655611-15-4158-4204
    - ⏱ 2020-04-12 11:23:37 

- 📌 如果我们将字母换成对神经网络更有意义的单词，那么会发生什么情况呢？虽然第二个矩阵是2乘以1的矩阵，但是乘法规则是相同的。 ^22655611-15-4371-4431
    - ⏱ 2020-04-12 11:24:08 

- 📌 这真是太棒了！只要努力一点，理解矩阵乘法，就可以找到如此强大的工具，这样我们无需花费太多精力就可以实现神经网络了。 ^22655611-15-5941-5998
    - ⏱ 2020-04-12 11:31:25 

- 📌 激活函数其实很简单，并不需要矩阵乘法。我们所需做的，是对矩阵X的每个单独元素应用S函数y=1 /（1+e -x）。 ^22655611-15-6044-6184
    - ⏱ 2020-04-12 11:32:00 
## 1.9 使用矩阵乘法的三层神经网络示例


- 📌 在Xhidden层中的元素上应用S函数，生成中间隐藏层输出矩阵。 ^22655611-16-4341-4426
    - ⏱ 2020-04-12 11:45:56 
## 1.10 学习来自多个节点的权重


- 📌 不等分误差。与前一种思想相反，我们为较大链接权重的连接分配更多的误差 ^22655611-17-1353-1387
    - ⏱ 2020-04-12 11:52:42 
## 1.11 多个输出节点反向传播误差


- 📌 1所分割的比例都等于4 /（4+4）=4/8=1/2。 ^22655611-18-2833-2867
    - ⏱ 2020-04-12 12:40:15 
## 1.12 反向传播误差到更多层中


- 📌 ，那么我们就从最终输出层往回工作，对每一层重复应用相同的思路。误差信息流具有直观意义。同样，你明白为什么我们称之为误差的反向传播了 ^22655611-19-1281-1346
    - ⏱ 2020-04-12 12:46:38 

- 📌 隐藏层的每个节点确实有一个单一的输出。还记得，我们在该节点经过加权求和的信号上应用激活函数，才得到了这个输出。但是，我们如何才能计算出误差呢？ ^22655611-19-1525-1596
    - ⏱ 2020-04-12 12:49:44 

- 📌 我们没有目标值或所希望的输出值。我们只有最终输出层节点的目标值，这个目标值来自于训练样本数据。 ^22655611-19-1634-1681
    - ⏱ 2020-04-12 12:51:10 

- 📌 使用先前所看到的误差反向传播，为链接重组分割的误差。 ^22655611-19-2374-2400
    - ⏱ 2020-04-12 12:54:05 
## 1.14 我们实际上如何更新权重


- 📌 们使用误差来指导如何调整链接权重，从而改进神经网络输出的总体答案 ^22655611-21-625-657
    - ⏱ 2020-04-13 15:00:55 

- 📌 S阈值函数，将所得到的结果输出给下一层的节点 ^22655611-21-761-783
    - ⏱ 2020-04-13 15:01:30 

- 📌 这种方法称为梯度下降（gradient descent）， ^22655611-21-3487-3516
    - ⏱ 2020-04-13 15:07:59 

- 📌 如果函数非常困难，我们不能用代数轻松找到最小值，我们就可以使用这个方法来代替代数方法。 ^22655611-21-3736-3779
    - ⏱ 2020-04-13 15:08:30 

- 📌 y表示误差，我们希望找到x，可以最小化y。 ^22655611-21-4236-4320
    - ⏱ 2020-04-13 15:09:11 

- 📌 要应用梯度下降的方法，我们必须找一个起点。上图显示了随机选择的起点。就像登山者，我们正站在这个地方，环顾四周，观察哪个方向是向下的 ^22655611-21-4546-4611
    - ⏱ 2020-04-13 15:09:32 

- 📌 这样就会避免在最小值的地方来回反弹 ^22655611-21-5192-5209
    - ⏱ 2020-04-13 15:10:32 

- 📌 ，我们从山上的不同点开始，多次训练神经网络，确保并不总是终止于错误的山谷 ^22655611-21-6788-6824
    - ⏱ 2020-04-13 15:13:16 

- 📌 起始点意味着选择不同的起始参数 ^22655611-21-6828-6843
    - ⏱ 2020-04-13 15:13:21 

- 📌 不同的起始链接权重。 ^22655611-21-6860-6870
    - ⏱ 2020-04-13 15:13:26 

- 📌 我们更喜欢使用第三种误差函数 ^22655611-21-8532-8546
    - ⏱ 2020-04-13 15:15:32 

- 📌 越接近最小值，梯度越小，这意味着，如果我们使用这个函数调节步长，超调的风险就会变得较小 ^22655611-21-8734-8777
    - ⏱ 2020-04-13 15:16:11 

- 📌 这就是我们一直在寻找的神奇表达式，也是训练神经网络的关键。 ^22655611-21-14751-14780
    - ⏱ 2020-04-13 15:19:34 
## 2.4 使用Python制作神经网络


- 📌 优秀的程序员、计算机科学家和数学家，只要可能，都尽力创建一般代码，而不是具体的代码。这是一种好习惯，它迫使我们以一种更深更广泛的适用方式思考求解问题 ^22655611-28-2356-2430
    - ⏱ 2020-04-13 16:26:12 

- 📌 我们希望同一个类可以创建一个小型的神经网络，也可创建一个大型的神经网络—只需传递所需的大小给参数即可。 ^22655611-28-2538-2589
    - ⏱ 2020-04-13 16:28:46 
# 读书笔记

# 本书评论
