---
doc_type: weread-highlights-reviews
bookId: "31418040"
author: 卡蒂克·雷迪·博卡
cover: https://wfqqreader-1252317822.image.myqcloud.com/cover/40/31418040/t7_31418040.jpg
reviewCount: 0
noteCount: 30
isbn: 9787111653578
category: 计算机-软件学习
lastReadDate: 2021-02-03
---
# 元数据
> [!abstract] 基于深度学习的自然语言处理
> - ![ 基于深度学习的自然语言处理|200](https://wfqqreader-1252317822.image.myqcloud.com/cover/40/31418040/t7_31418040.jpg)
> - 书名： 基于深度学习的自然语言处理
> - 作者： 卡蒂克·雷迪·博卡
> - 简介： 将深度学习方法应用于各种自然语言处理任务可以将你的计算算法在速度和准确性方面提升到一个全新的水平。本书首先介绍自然语言处理领域的基本构件，接着介绍了使用最先进的神经网络模型可以解决的问题。随着学习的深入，读者将学习卷积神经网络、递归神经网络和迭代神经网络，此外还包括长期短期记忆网络（LSTM）。在后面的章节中，读者将能够使用自然语言处理技术开发应用程序，例如注意力模型和集束搜索（Beam Search）。
> - 出版时间 2020-05-01 00:00:00
> - ISBN： 9787111653578
> - 分类： 计算机-软件学习
> - 出版社： 机械工业出版社

# 高亮划线

## 5.1 本章概览


- 📌 这非常重要，作为人类，我们能够做到这一点是因为我们的大脑能够存储记忆，分析过去的数据，检索有用的信息以理解当前的场景。 ^31418040-53-756-815
    - ⏱ 2021-02-03 10:18:03 

- 📌 苹果的Siri、谷歌的语音助理，还是微软的Cortana，它们所有的语音识别系统都使用RNN。 ^31418040-53-1215-1262
    - ⏱ 2021-02-03 10:19:15 
## 5.5.4 有状态与无状态


- 📌 无状态模式基本上是说一个批处理中的一个例子与下一个批处理中的任何例子都不相关。 ^31418040-66-529-568
    - ⏱ 2021-02-03 19:17:55 
## 6.1 本章概览


- 📌 门控循环单元 ^31418040-69-1069-1075
    - ⏱ 2021-02-03 19:25:31 
## 6.2 简单RNN的缺点


- 📌 也就是说，前面的层会学习得太快，每次训练迭代之间的值会有很大的偏差，而后面的层的梯度变化不会很快。 ^31418040-70-3226-3275
    - ⏱ 2021-02-03 19:31:23 

- 📌 梯度裁剪 ^31418040-70-3567-3571
    - ⏱ 2021-02-03 19:32:00 
## 6.3 门控循环单元


- 📌 残差网络 ^31418040-71-577-581
    - ⏱ 2021-02-03 19:33:03 

- 📌 跨层更均匀地学习 ^31418040-71-626-634
    - ⏱ 2021-02-03 19:33:12 
## 6.4 基于GRU的情感分析


- 📌 简单的标记化或TFIDF ^31418040-77-1036-1044
    - ⏱ 2021-02-03 19:41:50 

- 📌 我们需要使用该模型实际生成一些文本，如下所示： ^31418040-77-11503-11526
    - ⏱ 2021-02-03 19:47:40 
## 6.5 本章小结


- 📌 它通过允许模型学习文本结构中的长期依赖关系来帮助解决梯度消失的问题 ^31418040-78-425-458
    - ⏱ 2021-02-03 19:48:32 
## 7.1.1 LSTM


- 📌 单元状态 ^31418040-81-1844-1848
    - ⏱ 2021-02-03 19:54:25 

- 📌 字母“C”表示 ^31418040-81-1852-1859
    - ⏱ 2021-02-03 19:54:28 

- 📌 一条穿过不同时间实例并携带一些信息的传送带。 ^31418040-81-1894-1916
    - ⏱ 2021-02-03 19:54:37 

- 📌 来自先前单元状态的信息很容易到达下一个单元状态 ^31418040-81-1941-1964
    - ⏱ 2021-02-03 19:54:50 
## 7.1.2 遗忘门


- 📌 遗忘门负责确定应在前一个时间步长中遗忘的单元状态的内容 ^31418040-82-412-439
    - ⏱ 2021-02-03 19:58:29 

- 📌 以另一个新的权重集U_f，其维度 ^31418040-82-778-794
    - ⏱ 2021-02-03 20:03:00 

- 📌 ，因此有必要忘记单元状态中代表性别的旧值 ^31418040-82-1525-1545
    - ⏱ 2021-02-03 20:03:46 
## 7.3 输出门和当前激活


- 📌 为当前状态生成激活h[t] ^31418040-84-444-457
    - ⏱ 2021-02-03 20:11:00 

- 📌 输出门 ^31418040-84-462-465
    - ⏱ 2021-02-03 20:11:04 

- 📌 时间步长（h[t-1]）的激活 ^31418040-84-793-808
    - ⏱ 2021-02-03 20:11:26 

- 📌 构建一个非常简单的二进制分类器来演示LSTM的用法 ^31418040-84-3366-3391
    - ⏱ 2021-02-03 20:13:28 

- 📌 我们现在可以将标签分布映射到0/1，这样它就可以被馈送到分类器 ^31418040-84-5814-5845
    - ⏱ 2021-02-03 20:14:46 

- 📌 所以只有文本中属于前100个最频繁的单词才会被分配一个整数索引。其余的工作将被忽略。因此，即使X中的第一个序列有20个单词，在这个句子的标记化表示中有6个索引。 ^31418040-84-7227-7307
    - ⏱ 2021-02-03 20:18:25 

- 📌 'pre'（预）模式下完成的，这意味着序列的初始部分被填充以使序列长度等于max_len ^31418040-84-7896-7938
    - ⏱ 2021-02-03 20:18:54 

- 📌 64个隐藏单元的LSTM层 ^31418040-84-7977-7990
    - ⏱ 2021-02-03 20:19:02 
## 7.4 神经语言翻译


- 📌 必须使用更复杂的单元，如LSTM。神经语言翻译就是这样一种应用 ^31418040-85-492-523
    - ⏱ 2021-02-03 20:23:15 

- 📌 将一段文本从源语言翻译成目标语言 ^31418040-85-572-588
    - ⏱ 2021-02-03 20:23:20 

- 📌 系统有许多输入，系统也产生许多输出 ^31418040-85-666-683
    - ⏱ 2021-02-03 20:23:26 
## 9.4 谷歌Colab


- 📌 它为任何想要训练深度学习模式的人提供每天12小时的免费GPU使用。 ^31418040-109-680-713
    - ⏱ 2021-01-31 09:32:45 
# 读书笔记

# 本书评论
