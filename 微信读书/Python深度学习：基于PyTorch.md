---
doc_type: weread-highlights-reviews
bookId: "26858491"
author: 吴茂贵
cover: https://wfqqreader-1252317822.image.myqcloud.com/cover/491/26858491/t7_26858491.jpg
reviewCount: 0
noteCount: 108
isbn: 9787111637172
category: 计算机-编程设计
lastReadDate: 2021-07-11
---
# 元数据
> [!abstract] Python深度学习：基于PyTorch
> - ![ Python深度学习：基于PyTorch|200](https://wfqqreader-1252317822.image.myqcloud.com/cover/491/26858491/t7_26858491.jpg)
> - 书名： Python深度学习：基于PyTorch
> - 作者： 吴茂贵
> - 简介： 深度学习是一块难啃的硬骨头，对有一定开发经验和数学基础的从业者是这样，对初学者更是如此。其中卷积神经网络、循环神经网络、对抗式神经网络是深度学习的基石，同时也是深度学习的3大硬骨头。为了让读者更好地理解掌握这些网络，我们采用循序渐进的方式，先从简单特例开始，然后逐步介绍更一般性的内容，最后通过一些PyTorch代码实例实现之，整本书的结构及各章节内容安排都遵循这个原则。此外，一些优化方法也采用这种方法，如对数据集Cifar10分类优化，先用一般卷积神经网络，然后使用集成方法、现代经典网络，最后采用数据增加和迁移方法，使得模型精度不断提升，由最初的68%，上升到74%和90%，最后达到95%左右。
> - 出版时间 2019-10-01 00:00:00
> - ISBN： 9787111637172
> - 分类： 计算机-编程设计
> - 出版社： 机械工业出版社

# 高亮划线

## 前言


- 📌 PyTorch是动态计算图，其用法更贴近Python，并且，PyTorch与Python共用了许多Numpy的命令，可以降低学习的门槛，比TensorFlow更容易上手 ^26858491-3-739-823
    - ⏱ 2020-07-24 00:59:32 
## 1.1 生成Numpy数组


- 📌 使用shuffle函数打乱生成的随机数。 ^26858491-6-2647-2667
    - ⏱ 2020-08-03 20:07:09 

- 📌 有时还可能需要把生成的数据暂时保存起来，以备后续使用。 ^26858491-6-4008-4035
    - ⏱ 2020-08-03 19:56:51 
## 2.1 为何选择PyTorch？


- 📌 表征张量（或多维数组） ^26858491-15-461-472
    - ⏱ 2020-08-03 20:46:42 

- 📌 torch：类似于Numpy的通用数组库，可将张量类型转换为torch.cuda.TensorFloat，并在GPU上进行计算。·torch.autograd：用于构建计算图形并自动获取梯度的包。·torch.nn：具有共享层和损失函数的神经网络库。·torch.optim：具有通用优化算法（如SGD、Adam等）的优化包。 ^26858491-15-1317-1573
    - ⏱ 2020-08-03 21:01:03 
## 2.2 安装配置


- 📌 CUDA（Compute Unified Device Architecture），是英伟达公司推出的一种基于新的并行编程模型和指令集架构的通用计算架构，它能利用英伟达GPU的并行计算引擎，比CPU更高效地解决许多复杂计算任务。 ^26858491-16-3392-3506
    - ⏱ 2020-08-03 22:21:39 
## 2.3 Jupyter Notebook环境配置


- 📌 · ^26858491-17-841-842
    - ⏱ 2020-09-05 14:03:12 
## 2.4 Numpy与Tensor


- 📌 1）不修改自身数据，如x.add(y)，x的数据不变，返回一个新的Tensor。2）修改自身数据，如x.add_(y)（运行符带下划线后缀），运算结果存在x中，x被修改。 ^26858491-18-1092-1206
    - ⏱ 2020-09-05 14:17:23 

- 📌 torch.Tensor使用全局默认dtype（FloatTensor），而torch.tensor是从数据中推断 ^26858491-18-2315-2372
    - ⏱ 2020-09-05 17:26:53 

- 📌 torch.tensor(1)返回一个固定值1，而torch.Tensor(1)返回一个大小为1的张量，它是随机初始化的值。 ^26858491-18-2408-2470
    - ⏱ 2020-09-05 17:27:01 

- 📌 就是对输入进行归并或合计等操作，这类操作的输入输出形状一般并不相同，而且往往是输入大于输出形状 ^26858491-18-6821-6868
    - ⏱ 2020-09-05 20:05:30 

- 📌 归并操作可以对整个Tensor，也可以沿着某个维度进行归并。常见的归并操作可参考表2-5。 ^26858491-18-6869-6914
    - ⏱ 2020-09-05 20:07:13 

- 📌 mm是对2D的矩阵进行点积，bmm对含batch的3D进行点积运算。 ^26858491-18-8732-8766
    - ⏱ 2020-09-05 20:33:51 
## 2.5 Tensor与Autograd


- 📌 使用requires_grad参数指定是否记录对其的操作，以便之后利用backward()方法进行梯度求解。requires_grad参数的缺省值为False，如果要对其求导需设置为True，然后与之有依赖关系的节点会自动变为True ^26858491-19-854-968
    - ⏱ 2020-09-05 20:52:13 

- 📌 可以调用.detach()或with torch.no_grad()：，将不再计算张量的梯度，跟踪张量的历史记录。这点在评估 ^26858491-19-1049-1111
    - ⏱ 2020-09-05 20:52:49 

- 📌 可以通过用torch.no_grad()包裹代码块的形式来阻止autograd去跟踪那些标记 ^26858491-19-1658-1704
    - ⏱ 2020-09-05 20:58:17 

- 📌 其中x、w、b为变量，是用户创建的变量，不依赖于其他变量，故又称为叶子节点。 ^26858491-19-2081-2119
    - ⏱ 2020-09-05 21:08:51 

- 📌 y、z是计算得到的变量，非叶子节点，z为根节点。mul和add是算子（或操作或函数） ^26858491-19-2178-2220
    - ⏱ 2020-09-05 21:09:02 

- 📌 根据复合函数导数的链式法则，不难算出各叶子节点的梯度。 ^26858491-19-2526-2553
    - ⏱ 2020-09-05 21:06:13 

- 📌 PyTorch调用backward()方法，将自动计算各节点的梯度，这是一个反向传播过程，这个过程可用图2-9表示 ^26858491-19-2926-2983
    - ⏱ 2020-09-05 21:06:26 

- 📌 从当前根节点z反向溯源，利用导数链式法则，计算所有叶子节点的梯度，其梯度值将累加到grad属性中 ^26858491-19-3010-3058
    - ⏱ 2020-09-05 21:08:12 

- 📌 对非叶子节点的计算操作（或Function）记录在grad_fn属性中，叶子节点的grad_fn值为None。 ^26858491-19-3059-3114
    - ⏱ 2020-09-05 21:09:19 

- 📌 假设x、w、b都是标量，z=wx+b，对标量z调用backward()方法，我们无须对backward()传入参数 ^26858491-19-3484-3541
    - ⏱ 2020-09-05 21:36:43 

- 📌 [插图] ^26858491-19-4113-4114
    - ⏱ 2021-07-11 15:14:25 

- 📌 PyTorch有个简单的规定，不让张量（Tensor）对张量求导，只允许标量对张量求导，因此，如果目标张量对一个非标量调用backward()，则需要传入一个gradient参数，该参数也是张量，而且需要与调用backward()的张量形状相 ^26858491-19-5453-5574
    - ⏱ 2020-09-05 22:12:19 

- 📌 传入这个参数就是为了把张量对张量的求导转换为标量对张量的求导。这 ^26858491-19-5627-5659
    - ⏱ 2020-09-05 22:12:37 

- 📌 由此可见，错在v的取值，通过这种方式得到的并不是y对x的梯度。这里我们可以分成两步计算。首先让v=(1,0)得到y1对x的梯度，然后使v=(0,1)，得到y2对x的梯度。这里因需要重复使用backward()，需要使参数retain_graph=True，具体代码如下： ^26858491-19-8128-8313
    - ⏱ 2020-09-05 22:04:32 
## 2.6 使用Numpy实现机器学习


- 📌 随机数种子，生成同一个份 ^26858491-20-1225-1237
    - ⏱ 2020-08-11 10:38:42 
## 2.8 使用TensorFlow架构


- 📌 而PyTorch的动态图，动态的最关键的一点就是它是交互式的，而且执行每个命令马上就可看到结果，这对训练、发现问题、纠正问题非常方便，且其构图是一个叠加（动态）过程，期间我们可以随时添加内容。这些特征对于训练和调试过程无疑是非常有帮助的，这或许也是PyTorch为何在高校、科研院所深得使用者喜爱的重要原因。 ^26858491-22-3181-3335
    - ⏱ 2020-09-06 13:00:20 
## 3.2 实现神经网络实例


- 📌 ？像卷积层、全连接层、Dropout层等因含有可学习参数，一般使用nn.Module，而激活函数、池化层不含可学习参数，可以使用nn.functional中对应的函数 ^26858491-26-656-739
    - ⏱ 2020-09-08 08:03:23 

- 📌 Normalize([0.5],[0.5])对张量进行归一化，这里两个0.5分别表示对张量进行归一化的全局平均值和方差。因图像是灰色的只有一个通道，如果有多个通道，需要有多个数字，如3个通道，应该是Normalize([m1,m2,m3],[n1,n2,n3])； ^26858491-26-3257-3389
    - ⏱ 2020-09-08 08:09:05 

- 📌 训练模型，这里使用for循环，进行迭代。其中包括对训练数据的训练模型，然后用测试数据的验证模型。 ^26858491-26-5529-5577
    - ⏱ 2020-09-09 11:00:44 

- 📌 [插图] ^26858491-26-5622-5623
    - ⏱ 2020-09-21 16:22:48 
## 3.3 如何构建神经网络？


- 📌 采用了torch.nn.Sequential()来构建网络层，这个有点类似Keras的models.Sequential()，使用起来就像搭积木一样，非常方便 ^26858491-27-634-713
    - ⏱ 2020-09-09 11:10:28 

- 📌 forward函数的任务需要把输入层、网络层、输出层链接起来，实现信息的前向传导。该函数的参数一般为输入数据，返回值为输出数 ^26858491-27-1776-1838
    - ⏱ 2020-09-09 11:12:44 

- 📌 在forward函数中，有些层来自nn.Module，也可以使用nn.functional定义。来自nn.Module的需要实例化，而使用nn.functional定义的可以直接使用。 ^26858491-27-1869-1961
    - ⏱ 2020-09-09 11:13:05 

- 📌 深度学习中涉及很多函数，如果要自己手工实现反向传播，比较费时。好在PyTorch提供了自动反向传播的功能，使用nn工具箱，无须我们自己编写反向传播，直接让损失函数（loss）调用backward()即可，非常方便和高效！ ^26858491-27-2117-2227
    - ⏱ 2020-09-09 11:13:59 

- 📌 接下来就是训练模型。训练模型时需要注意使模型处于训练模式，即调用model.train()。调用model.train()会把所有的module设置为训练模式。如果是测试或验证阶段，需要使模型处于验证阶段，即调 ^26858491-27-2446-2551
    - ⏱ 2020-09-09 11:15:09 

- 📌 缺省情况下梯度是累加的，需要手工把梯度初始化或清零，调用optimizer.zero_grad()即可。 ^26858491-27-2632-2684
    - ⏱ 2020-09-09 11:15:26 

- 📌 计算输出和实际值之间的损失值。调用loss.backward()自动生成梯度，然后使用optimizer.step（）执行优化器，把梯度传播回每个网络。 ^26858491-27-2702-2778
    - ⏱ 2020-09-09 11:16:13 

- 📌 如果希望用GPU训练，需要把模型、训练数据、测试数据发送到GPU上，即调用.to(device) ^26858491-27-2807-2855
    - ⏱ 2020-09-09 11:16:24 

- 📌 如果需要使用多GPU进行处理，可使模型或相关数据引用nn.DataParallel。nn.DataParallel的具体使用在第4章将详细介绍 ^26858491-27-2856-2927
    - ⏱ 2020-09-09 11:16:34 
## 3.4 神经网络工具箱nn


- 📌 在nn工具箱中有两个重要模块：nn.Model、nn.functional，接下来将介绍这两个模块。 ^26858491-28-545-595
    - ⏱ 2020-09-09 11:17:08 

- 📌 在实际使用中，最常见的做法是继承nn.Module，生成自己的网络/层， ^26858491-28-749-785
    - ⏱ 2020-09-09 11:18:48 

- 📌 nn.Xxx继承于nn.Module，nn.Xxx需要先实例化并传入参数， ^26858491-28-1366-1403
    - ⏱ 2020-09-09 11:19:55 

- 📌 nn.Xxx不需要自己定义和管理weight、bias参数 ^26858491-28-1524-1553
    - ⏱ 2020-09-09 11:20:49 
## 3.5 优化器


- 📌 使用的随机梯度下降法（SGD）就是最普通的优化器 ^26858491-29-581-605
    - ⏱ 2020-09-09 11:22:41 

- 📌 在梯度反向传播前，先需把梯度清零。 ^26858491-29-1286-1303
    - ⏱ 2020-09-09 11:25:54 

- 📌 基于当前梯度（存储在参数的.grad属性中）更新参数。 ^26858491-29-1515-1542
    - ⏱ 2020-09-09 11:26:03 
## 3.6 动态修改学习率参数


- 📌 改参数optimizer.params_groups。 ^26858491-30-556-583
    - ⏱ 2020-09-09 11:27:19 
## 3.7 优化器比较


- 📌 不过，像自适应优化器在深度学习中比较受欢迎，除了性能较好，鲁棒性、泛化能力也更强。 ^26858491-31-426-467
    - ⏱ 2020-09-09 11:28:47 
## 第4章 PyTorch数据处理工具箱


- 📌 简单介绍PyTorch相关的数据处理工具箱。·utils.data简介。·torchvision简介。·tensorboardX简介及实例。 ^26858491-33-648-811
    - ⏱ 2020-09-09 11:34:39 
## 4.1 数据处理工具箱概述


- 📌 中间是PyTorch可视化处理工具（Torchvision） ^26858491-34-1142-1172
    - ⏱ 2020-09-09 13:48:20 

- 📌 utils：含两个函数，一个是make_grid，它能将多张图片拼接在一个网格中；另一个是save_img，它能将Tensor保存成图片。 ^26858491-34-1694-1763
    - ⏱ 2020-09-09 13:52:21 
## 4.2 utils.data简介


- 📌 自定义数据集需要继承这个类，并实现两个函数，一个是__len__，另一个是__getitem__，前者提供数据的大小（size），后者通过给定索引获取数据和标签。__getitem__一次只能获取一个数据，所以需要通过torch.utils.data.DataLoader来定义一个新的迭代器，实现batch读取。 ^26858491-35-462-619
    - ⏱ 2020-09-09 14:09:01 

- 📌 以上数据以tuple返回，每次只返回一个样本。实际上，Dateset只负责数据的抽取，调用一次__getitem__只返回一个样本。如果希望批量处理（batch），还要同时进行shuffle和并行加速等操作，可选择DataLoader。 ^26858491-35-1542-1660
    - ⏱ 2020-09-09 14:22:50 

- 📌 num_workers：使用多进程加载的进程数，0代表不使用多进程。 ^26858491-35-2236-2270
    - ⏱ 2020-09-09 14:25:51 

- 📌 dataset中的数据个数可能不是batch_size的整数倍，drop_last为True会将多出来不足一个batch的数据丢弃。 ^26858491-35-2476-2542
    - ⏱ 2020-09-09 14:26:14 

- 📌 不过由于它不是迭代器，我们可以通过iter命令将其转换为迭代器。 ^26858491-35-3084-3116
    - ⏱ 2020-09-09 14:37:52 

- 📌 如果数据在不同目录下，因为不同的目录代表不同类别（这种情况比较普遍），使用data.Dataset来处理就很不方便。不过，使用PyTorch另一种可视化数据处理工具（即torchvision）就非常方便，不但可以自动获取标签，还提供很多数据预处理、数据增强等转换函数。 ^26858491-35-3241-3375
    - ⏱ 2020-09-09 14:38:26 
## 4.3 torchvision简介


- 📌 本节主要介绍如何使用datasets的ImageFolder处理自定义数据集 ^26858491-36-511-549
    - ⏱ 2020-09-09 14:41:39 

- 📌 Normalize：标准化，即，减均值，除以标准差。 ^26858491-36-1452-1478
    - ⏱ 2020-09-09 14:55:02 

- 📌 将Tensor转为PIL Image。 ^26858491-36-1521-1540
    - ⏱ 2020-09-09 15:12:00 

- 📌 果要对数据集进行多个操作，可通过Compose将这些操作像管道一样拼接起来 ^26858491-36-1570-1607
    - ⏱ 2020-09-09 15:15:15 
## 4.4 可视化工具


- 📌 为解决这一问题，人们推出了可用于PyTorch可视化的新的更强大的工具——tensorboardX。 ^26858491-37-544-594
    - ⏱ 2020-09-09 15:18:56 
## 5.1 机器学习的基本任务


- 📌 自编码器是一种半监督学习，其生成的目标就是未经修改的输入。语言处理中根据给定文本中的词预测下一个词，也是半监督学习的例子。 ^26858491-41-1701-1762
    - ⏱ 2020-09-09 15:35:48 

- 📌 对抗生成式网络也是一种半监督学习，给定一些真图片或语音，然后通过对抗生成网络生成一些与真图片或是语音逼真的图形或语音。 ^26858491-41-1791-1850
    - ⏱ 2020-09-09 15:35:57 
## 5.2 机器学习一般流程


- 📌 优化的方法很多，其中网格搜索参数是一种有效方法 ^26858491-42-3599-3622
    - ⏱ 2020-09-09 15:51:24 

- 📌 尤其是回归类的问题，可以考虑正则化的方法来降低模型的泛化误差。 ^26858491-42-3650-3681
    - ⏱ 2020-09-09 15:51:29 
## 5.3 过拟合与欠拟合


- 📌 如果要降低模型的复杂度，可以通过缩减它们的系数来实现，如把第3次、4次项的系数θ3、θ4缩减到接近于0即可。 ^26858491-43-1651-1755
    - ⏱ 2020-09-09 16:00:13 

- 📌 希望通过上面的简单介绍，能让读者有个直观理解。传统意义上的正则化一般分为L0、L1、L2、L∞等。 ^26858491-43-3074-3223
    - ⏱ 2020-09-09 16:01:20 
## 5.4 选择合适激活函数


- 📌 激活函数在神经网络中作用有很多，主要作用是给神经网络提供非线性建模能力 ^26858491-44-398-433
    - ⏱ 2020-09-09 16:08:30 

- 📌 所以层数较多的激活函数需要考虑其导数不宜小于1当然也不能大于1，大于1将导致梯度爆炸，导数为1最好，而激活函数relu正好满足这个条件 ^26858491-44-961-1028
    - ⏱ 2020-09-09 16:09:54 
## 5.5 选择合适的损失函数


- 📌 损失函数用来衡量模型的好坏，损失函数越小说明模型和参数越符合训练样本。任何能够衡量模型预测值与真实值之间的差异的函数都可以叫作损失函数。 ^26858491-45-502-570
    - ⏱ 2020-09-09 16:11:26 

- 📌 损失函数有两种，即交叉熵(Cross Entropy)和均方误差（Mean squared error，MSE），分别对应机器学习中的分类问题和回归问题。 ^26858491-45-579-656
    - ⏱ 2020-09-09 16:11:47 

- 📌 对分类问题的损失函数一般采用交叉熵，交叉熵反应的两个概率分布的距离 ^26858491-45-685-718
    - ⏱ 2020-09-09 16:12:08 
## 5.6 选择合适优化器


- 📌 影响优化的无非两个因素：一个是梯度方向，一个是学习率。 ^26858491-46-1926-1953
    - ⏱ 2020-09-09 16:18:15 

- 📌 既然每一步都要将两个梯度方向（历史梯度、当前梯度）做一个合并再下降 ^26858491-46-3223-3256
    - ⏱ 2020-09-10 09:26:25 

- 📌 RMSProp算法通过修改AdaGrad得来，其目的是在非凸背景下效果更好。针对梯度平方和累计越来越大的问题，RMSProp指数加权的移动平均代替梯度平方和 ^26858491-46-5577-5655
    - ⏱ 2020-09-20 15:46:57 

- 📌 使用SGD时，必须手动选择学习率和动量参数，通常会随着时间的推移而降低学习率 ^26858491-46-6847-6885
    - ⏱ 2020-09-23 16:28:08 

- 📌 有时可以考虑综合使用这些优化算法，如采用先使用Adam，然后使用SGD的优化方法，这个想法，实际上是由于在训练的早期阶段SGD对参数调整和初始化非常敏感。因此，我们可以通过先使用Adam优化算法来进行训练，这将大大地节省训练时间，且不必担心初始化和参数调整，一旦用Adam训练获得较好的参数后，就可以切换到SGD+动量优化，以达到最佳性能。采用这种方法有时能达到很好的效果，如图5-17所示，迭代次数超过150后，用SGD效果好于Adam。 ^26858491-46-6915-7135
    - ⏱ 2020-09-10 09:40:36 
## 5.7 GPU加速


- 📌 程序在GPU系统上的运行速度相较于单 ^26858491-47-678-696
    - ⏱ 2020-08-19 09:28:11 

- 📌 法.to(device)或.cuda()。 ^26858491-47-1813-1834
    - ⏱ 2020-09-10 09:56:25 

- 📌 然后用data.DataLoader转换为可批加载的方式。采用nn.DataParallel并发机制，环境有2个GPU ^26858491-47-3300-3357
    - ⏱ 2020-09-10 10:00:58 

- 📌 通过web查看损失值的变化情况，如图5-20所示。 ^26858491-47-7155-7180
    - ⏱ 2020-08-19 09:31:29 
## 6.2 卷积层


- 📌 对卷积直观的理解，就是两个函数的一种运算， ^26858491-51-433-454
    - ⏱ 2020-08-17 16:25:51 

- 📌 过滤器一般是奇数阶矩阵 ^26858491-51-1788-1799
    - ⏱ 2020-09-10 11:43:56 

- 📌 在深度学习中，过滤器的作用不仅在于检测垂直边缘、水平边缘等，还需要检测其他边缘特征。 ^26858491-51-2816-2858
    - ⏱ 2020-08-17 17:34:36 

- 📌 过滤器类似于标准神经网络中的权重矩阵W，W需要通过梯度下降算法反复迭代求得 ^26858491-51-2896-2933
    - ⏱ 2020-08-17 17:34:56 

- 📌 所以又把卷积核的值称为共享变量。卷积神经网络采用参数共享的方法大大降低了参数的数量。 ^26858491-51-4343-4385
    - ⏱ 2020-09-10 11:50:37 

- 📌 当输入图片与卷积核不匹配时或卷积核超过图片边界时，可以采用边界填充（Padding）的方法。即把图片尺寸进行扩展，扩展区域补零 ^26858491-51-4920-4983
    - ⏱ 2020-09-10 11:52:03 

- 📌 我们再把4×4的输入特征展成[16,1]的矩阵X，那么Y=CX则是一个[4,1]的输出特征矩阵，把它重新排列2×2的输出特征就得到最终的结果， ^26858491-51-11029-11100
    - ⏱ 2020-09-13 21:13:56 

- 📌 可得X=CTY，即反卷积的操作就是要对这个矩阵运算过程进行逆运算。 ^26858491-51-11424-11484
    - ⏱ 2020-09-13 21:10:33 
## 6.3 池化层


- 📌 在PyTorch中，最大池化常使用nn.MaxPool2d，平均池化使用nn.AvgPool2d。 ^26858491-52-1483-1532
    - ⏱ 2020-09-21 14:46:05 
## 6.4 现代经典网络


- 📌 残差网络的核心思想即：输出的是两个连续的卷积层，并且输入时绕到下一层去 ^26858491-53-4966-5001
    - ⏱ 2020-09-14 10:18:54 

- 📌 胶囊网络克服了卷积神经网络的一些不足：1）训练卷积神经网络一般需要较大数据量，而胶囊网络使用较少数据就能泛化。2）卷积神经网络因池化层、全连接层等丢失大量的信息，从而降低了空间位置的分辨率，而胶囊网络对很多细节的姿态信息（如对象的准确位置、旋转、厚度、倾斜度、尺寸等）能在网络里被保存。这就有效地避免嘴巴和眼睛倒挂也认为是人脸的错误。3）卷积神经网络不能很好地应对模糊性，但胶囊网络可以。所以，它能在非常拥挤的场景中也表现得很好。 ^26858491-53-6173-6475
    - ⏱ 2020-09-14 10:01:05 
## 第7章 自然语言处理基础


- 📌 像这样与先后顺序有关的数据被称之为序列数据 ^26858491-58-545-566
    - ⏱ 2020-08-18 00:23:45 
## 7.1 循环神经网络基本结构


- 📌 它的共享参数方式是各个时间节点对应的W、U、V都是不变的， ^26858491-59-1126-1155
    - ⏱ 2020-09-18 10:06:05 
## 7.6 词嵌入


- 📌 把这种方式转换为独热表示方法，独热表示的向量长度为词典的大小，向量的分量只有一个1，其他全为0，1的位置对应该词在词典中的位置。例如： ^26858491-64-891-958
    - ⏱ 2020-08-19 08:21:03 

- 📌 对字或词转换为独热向量，而整篇文章则转换为一个稀疏矩阵 ^26858491-64-1105-1132
    - ⏱ 2020-08-19 08:23:25 
## 7.8 用LSTM预测股票行情


- 📌 立训练模型： ^26858491-66-4555-4561
    - ⏱ 2020-08-19 08:43:09 
## 12.1 DeepDream模型


- 📌 如何解释CNN如何学习特征的？ ^26858491-100-496-511
    - ⏱ 2020-08-20 11:18:56 

- 📌 使用梯度上升的方法可视化网络每一层的特征，即用一张噪声图像输入网络，但反向更新的时候不更新网络权重，而是更新初始图像的像素值，以这种“训练图像”的方式可视化网络。DeepDream正是以此为基础。 ^26858491-100-699-797
    - ⏱ 2020-08-20 11:19:19 

- 📌 构建损失函数时，不使用通常的交叉熵，而是最大化特征值的L2范数。使图像经过网络之后提取的特征更像网络隐含的特征。 ^26858491-100-1511-1592
    - ⏱ 2020-08-20 11:20:19 

- 📌 之所以要缩小，图像缩小是为了让图像的像素点调整后所得结果图案能显示的更加平滑。缩小二次后，把图像每个像素点当作参数，对它们求偏导，这样就可以知道如何调整图像像素点能够对给定网络层的输出产生最大化的刺激。 ^26858491-100-2356-2457
    - ⏱ 2020-08-20 11:22:37 

- 📌 通常使用预训练模型，这里取VGG19预训练模型。第二个问题则是用把这些特征最大化后展示在一张普通的图像上，该图像为星空图像。 ^26858491-100-2647-2709
    - ⏱ 2020-08-20 11:23:21 
## 第15章 强化学习


- 📌 强化学习就像这种前无古人的学习，没有预先给定标签或模板，只有不断尝试后的结果反馈，好或不好，成与不成等。这种学习带有创新性，比一般的模仿性机器学习确实更强大一些，这或许也是其名称来由吧。 ^26858491-116-715-808
    - ⏱ 2020-12-02 17:45:36 
## 15.1 强化学习简介


- 📌 一个完整的强化学习过程是从一开始什么都不懂，通过不断尝试，从错误或惩罚中学习，最后找到规律，学会达到目的的方法。 ^26858491-117-456-512
    - ⏱ 2020-12-02 17:45:55 
# 读书笔记

# 本书评论
